{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37b4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import fnmatch\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "# Change to your dataset directory\n",
    "os.chdir('D:/学习/9444dataset/dataset_remade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f87736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect paths to all images\n",
    "imagePatches = glob('**/*.png', recursive=True)\n",
    "\n",
    "# Separate image paths based on class\n",
    "patternZero = '*class0.png'\n",
    "patternOne = '*class1.png'\n",
    "classZero = fnmatch.filter(imagePatches, patternZero)\n",
    "classOne = fnmatch.filter(imagePatches, patternOne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4229d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for img in imagePatches:\n",
    "    if img in classZero:\n",
    "        y.append(0)\n",
    "    elif img in classOne:\n",
    "        y.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c44be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "images_df = pd.DataFrame()\n",
    "images_df[\"images\"] = imagePatches\n",
    "images_df[\"labels\"] = y\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train, val = train_test_split(images_df, stratify=images_df.labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7619ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image augmentation workflow\n",
    "workflow = iaa.Sequential([\n",
    "    iaa.CropAndPad(percent=(0.05,0.1)), \n",
    "    iaa.Resize(224),  \n",
    "    iaa.Affine(rotate=(-5, 5), \n",
    "               scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "               translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "               shear=(-2, 2))],  \n",
    "    random_order=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab60c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df_data, transform=None, imgaug_transforms=None):\n",
    "        super().__init__()\n",
    "        self.df = df_data.values\n",
    "        self.transform = transform\n",
    "        self.imgaug_transforms = imgaug_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path,label = self.df[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert color space from BGR to RGB\n",
    "        image = cv2.resize(image, (224,224))  # Resize images to 224x224\n",
    "        \n",
    "        # Apply imgaug augmentations\n",
    "        if self.imgaug_transforms is not None:\n",
    "            image = self.imgaug_transforms(image=image)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8cb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_epochs = 10\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d918166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "trans_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.RandomHorizontalFlip(), \n",
    "                                  transforms.RandomVerticalFlip(),\n",
    "                                  transforms.RandomRotation(20), \n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "trans_valid = transforms.Compose([transforms.ToPILImage(),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ce90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "dataset_train = MyDataset(df_data=train, transform=trans_train, imgaug_transforms=workflow)\n",
    "dataset_valid = MyDataset(df_data=val, transform=trans_valid)\n",
    "\n",
    "loader_train = DataLoader(dataset = dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "loader_valid = DataLoader(dataset = dataset_valid, batch_size=batch_size//2, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6462b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model\n",
    "model = models.resnet50(pretrained=False)  # If you want to use the pretrained model, set pretrained=True\n",
    "\n",
    "# Change the final layer to match the number of classes in the dataset\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eb71b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bedce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/130], Loss: 0.5784\n",
      "Epoch [2/10], Step [100/130], Loss: 0.2263\n",
      "Epoch [3/10], Step [100/130], Loss: 0.3690\n",
      "Epoch [4/10], Step [100/130], Loss: 0.2500\n",
      "Epoch [5/10], Step [100/130], Loss: 0.3019\n",
      "Epoch [6/10], Step [100/130], Loss: 0.2622\n",
      "Epoch [7/10], Step [100/130], Loss: 0.2889\n",
      "Epoch [8/10], Step [100/130], Loss: 0.2296\n",
      "Epoch [9/10], Step [100/130], Loss: 0.2337\n",
      "Epoch [10/10], Step [100/130], Loss: 0.1380\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(loader_train)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(loader_train):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba06ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in loader_valid:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "                 \n",
    "    print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet_model22222.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
